{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inspecting the meta kaggle dataset\n",
    "\n",
    "### looking for data that will be useful in AI search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading /Users/arad/repos/pp_kaggle_query/KernelVersionDatasetSources.csv\n",
      "downloading /Users/arad/repos/pp_kaggle_query/DatasetTasks.csv\n",
      "downloading /Users/arad/repos/pp_kaggle_query/Datasets.csv\n",
      "downloading /Users/arad/repos/pp_kaggle_query/DatasetTaskSubmissions.csv\n",
      "downloading /Users/arad/repos/pp_kaggle_query/DatasetTags.csv\n",
      "downloading /Users/arad/repos/pp_kaggle_query/DatasetVersions.csv\n",
      "downloading /Users/arad/repos/pp_kaggle_query/DatasetVotes.csv\n"
     ]
    }
   ],
   "source": [
    "# narrow down to \"Dataset\" in the name of the file\n",
    "import os\n",
    "\n",
    "dir = '/Users/arad/repos/pp_kaggle_query/'\n",
    "\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if 'Dataset' in file:\n",
    "        print(f\"downloading {dir}{file}\")\n",
    "        df_dict[file] = pd.read_csv(f\"{dir}{file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'e', 's', 't', '_', 's', 't', 'r', 'i', 'n', 'g']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(df_dict[df].columns)\n",
    "# [1,2,3]\n",
    "# \"[1,2,3]\"\n",
    "# \"list\" -> [\"l\",\"i\",...]\n",
    "test_string = \"test_string\"\n",
    "list(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_dict:\n",
    "    # print(df)\n",
    "    # print()\n",
    "    if 'DatasetName' in df_dict[df].columns:\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptions\n",
    "# title\n",
    "# size\n",
    "## DatasetVersions.csv\n",
    "\n",
    "# file format\n",
    "# download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1104888 entries, 0 to 1104887\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count    Dtype  \n",
      "---  ------                  --------------    -----  \n",
      " 0   Id                      1104888 non-null  int64  \n",
      " 1   DatasetId               1104888 non-null  int64  \n",
      " 2   DatasourceVersionId     1104888 non-null  int64  \n",
      " 3   CreatorUserId           1104888 non-null  int64  \n",
      " 4   LicenseName             1104888 non-null  object \n",
      " 5   CreationDate            1104888 non-null  object \n",
      " 6   VersionNumber           1030293 non-null  float64\n",
      " 7   Title                   1104887 non-null  object \n",
      " 8   Slug                    1104888 non-null  object \n",
      " 9   Subtitle                588510 non-null   object \n",
      " 10  Description             503725 non-null   object \n",
      " 11  VersionNotes            1040699 non-null  object \n",
      " 12  TotalCompressedBytes    1030331 non-null  float64\n",
      " 13  TotalUncompressedBytes  1030458 non-null  float64\n",
      "dtypes: float64(3), int64(4), object(7)\n",
      "memory usage: 118.0+ MB\n"
     ]
    }
   ],
   "source": [
    "versions = df_dict['DatasetVersions.csv']\n",
    "versions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = versions.loc[~versions['Description'].isna(),'Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The [American Community Survey](http://www.cen...\n",
       "1    In the mid-eighteenth to nineteenth centuries,...\n",
       "3    Throughout 2015, Hillary Clinton has been embr...\n",
       "4    It's no secret that US university students oft...\n",
       "5    US Social Security applications are a great wa...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arad/repos/pp_kaggle_query\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "def chat_completion(system_prompt,user_prompt):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "# chat_completion(\"please explain what a p-value is like im 5 years old. be concise.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for descr in descriptions[:20]:\n",
    "    print(descr)\n",
    "    print(\"*\"*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31952\n"
     ]
    }
   ],
   "source": [
    "# we will concatenate all 20 descriptions together\n",
    "# to form one big chunk of text\n",
    "# we will show this text to gpt4\n",
    "# along with a question: can you find me a dataset about food\n",
    "\n",
    "full_text = \" \".join(descriptions[:15])\n",
    "\n",
    "print(len(full_text))\n",
    "\n",
    "# this is not a good prompt\n",
    "eval_system_prompt = \"\"\"\n",
    "\n",
    "you will help me decide what dataset is the most suitable based on a description of my project\n",
    "\n",
    "\"\"\"\n",
    "eval_user_prompt = f\"\"\"\n",
    "\n",
    "given the following dataset descriptions, please tell me which one is best suited to my project\n",
    "\n",
    "my project an analysis of food items\n",
    "\n",
    "here are the descriptions: {full_text}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10500.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 42k characters in our full text\n",
    "# each token = 4 characters\n",
    "42000/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most suitable dataset for your project, which is an analysis of food items, would be the \"[Amazon Fine Food Reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews)\" dataset. \\n\\nThis dataset consists of 568,454 food reviews Amazon users left up to October 2012. It includes product and user information, ratings, and a plain text review. It also includes reviews from all the Amazon categories. \\n\\nThe columns in this dataset are as follows:\\n\\n- Id: Review ID \\n- ProductId: unique identifier for the product \\n- UserId: unique identifier for the user \\n- ProfileName: Profile name of the user who submitted the review\\n- HelpfulnessNumerator: number of users who found the review helpful \\n- HelpfulnessDenominator: number of users who indicated whether they found the review helpful \\n- Score: rating between 1 and 5 \\n- Time: timestamp for the review \\n- Summary: brief summary of the review \\n- Text: full text of the review  \\n\\nAnalyzing this dataset can provide insights into consumer preferences and trends in the food market, which can be beneficial for both business strategies and academic studies.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion(eval_system_prompt,eval_user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
